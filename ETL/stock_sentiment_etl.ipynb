{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccbeb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import psycopg2\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3d0c0",
   "metadata": {},
   "source": [
    "### Important: Manually Enter Database Names and Passwords Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd86257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating connection string components for SQLAlchemy\n",
    "host = '127.0.0.1'\n",
    "port = 5432\n",
    "user = 'postgres'\n",
    "passw = \"\"\n",
    "database = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b03e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile connection string components and create engine\n",
    "db_string = f\"postgresql+psycopg2://{user}:{passw}@{host}:{port}/{database}\"\n",
    "engine = create_engine(db_string, pool_recycle=3600)\n",
    "# Connect to server\n",
    "dbConnection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7c547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_etl():\n",
    "    # ticker list manually curated via top trending tickers from apewisdom.io in May 2022\n",
    "    ticker_ls = ['GME','TSLA','AAPL','TWTR','ATER','AMC','AMD','MSFT','HD','FB','AMZN','NU',\n",
    "                'DTE','GOOG','NFLX','SOFI','TA','BBIG','NVDA','AI','ZIM','VTNR','PLTR','RIVN',\n",
    "                'LUNA','TTWO','OXY','BABA','WMT','DWAC','CC','COO','LFG','RBLX','DIS','SE',\n",
    "                'SNDL','PARA','SP','XOM','ES','JD','PTON','WBD','TGT','OG','DM','DOW','TLRY',\n",
    "                'NIO','PANW','CVNA','JPM','IP','TD','GS','CLOV','PYPL','GM','IQ','MCD','MULN',\n",
    "                'DE','VZ','WW','EA','BB','CRSR','TG','WEBR','ABNB','OI','CHGG','INTC','HP',\n",
    "                'SQ','ROKU','PT','VXX','BYND','JBLU','GBTC','MS','DKNG','FCF','GE','BJ','LCID',\n",
    "                'CS','KO','WTI','RIG','RC','BA','MMAT','RKLB','UNH','RE','CSCO','VC','ZM','TIL',\n",
    "                'IBKR','CRM','IBM','ET','CP','MSTR','DAC','DTC','NYT','TH','BP','LMT','SM','ITT',\n",
    "                'BBBY','TTD','PBR','SPCE','UPST','AFRM','NG']\n",
    "    \n",
    "    # positive string based on manual labelling / analysis of pulled data\n",
    "    pos_strings = ['calls', 'buy', 'bought', 'buying', 'all in', 'to the moon', 'long',\n",
    "                  'up', 'green', 'rocket', 'pop', 'bullish', 'bull', 'rally', 'beat', 'beats',\n",
    "                  'crush', 'upswing', 'tendies', 'profit', 'undervalued', 'rallies']\n",
    "\n",
    "    neg_strings = ['puts', 'pain', 'sell', 'sold', 'short', 'down', 'tank', 'red', 'dip',\n",
    "                  'bearish', 'bear', 'miss', 'selloff', 'underperform', 'drop', 'downswing', \n",
    "                  'overvalued', 'fucked', 'screwed']\n",
    "    \n",
    "    # transform DF from one to many per ticker present in row text \n",
    "    def transform_to_many(df):\n",
    "        new_df = pd.DataFrame(columns = [\"comment_id\", \"ticker\", \"date\", \"username\", \"subreddit\", \"body\"])\n",
    "        for i in range(len(df.index)):\n",
    "            row = df.loc[i]\n",
    "            text = row.body\n",
    "\n",
    "            for ticker in ticker_ls:\n",
    "                search_string = fr\"\\b\\$?({ticker.lower()})\\b\"\n",
    "                mo = re.search(search_string, text.lower())\n",
    "                if mo:\n",
    "                    new_row = pd.DataFrame([[row.comment_id, ticker, row.date, row.username, row.subreddit, row.body]],\n",
    "                                          columns = [\"comment_id\", \"ticker\", \"date\", \"username\", \"subreddit\", \"body\"])\n",
    "                    new_df = pd.concat([new_df, new_row], sort=False)\n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    def add_sentiment_data(df):\n",
    "        # initialize nltk sentiment analyzer\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        # overall sentiment compound scores\n",
    "        overall_sent_comp = [sia.polarity_scores(txt)['compound'] for txt in df['body']]\n",
    "        # overall sentiment positive scores\n",
    "        overall_sent_pos = [sia.polarity_scores(txt)['pos'] for txt in df['body']]\n",
    "        # overall sentiment negative scores\n",
    "        overall_sent_neg = [sia.polarity_scores(txt)['neg'] for txt in df['body']]\n",
    "\n",
    "        # polarity scores for TOKENIZED sentences\n",
    "        sent_tokens = [nltk.sent_tokenize(txt) for txt in df['body']]\n",
    "        pol_scores = [[sia.polarity_scores(sent) for sent in txt] for txt in sent_tokens]\n",
    "\n",
    "        # mean scores for tokenized sentences\n",
    "        mean_t_comp_scores = [mean([token[\"compound\"] for token in item]) for item in pol_scores]\n",
    "        mean_t_pos_scores = [mean([token[\"pos\"] for token in item]) for item in pol_scores]\n",
    "        mean_t_neg_scores = [mean([token[\"neg\"] for token in item]) for item in pol_scores]  \n",
    "\n",
    "        # filter sentence tokens based on corresponding ticker\n",
    "        tickers = df['ticker']\n",
    "        tgt_tokens = [[sent for sent in txt if ticker.lower() in sent.lower()] for txt, ticker in zip(sent_tokens, tickers)]\n",
    "\n",
    "        # generate polarity scores based on targeted sentence tokens\n",
    "        target_pol_scores = [[sia.polarity_scores(sent) for sent in txt] for txt in tgt_tokens]\n",
    "        \n",
    "        # compute mean scores per token sentences\n",
    "        mean_tgt_comp_scores = [mean([token[\"compound\"] for token in item]) for item in target_pol_scores]\n",
    "        mean_tgt_pos_scores = [mean([token[\"pos\"] for token in item]) for item in target_pol_scores]\n",
    "        mean_tgt_neg_scores = [mean([token[\"neg\"] for token in item]) for item in target_pol_scores]\n",
    "\n",
    "\n",
    "        # estimate average verb tense of text \n",
    "        def determine_tense(sentence):\n",
    "            text = word_tokenize(sentence)\n",
    "            tagged = pos_tag(text)\n",
    "\n",
    "            tense = {}\n",
    "            tense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n",
    "            tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "            tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "            return(max(tense, key=tense.get))\n",
    "\n",
    "        # get maximum value of verb tense counter to estimate average verb tense\n",
    "        verb_tenses = [max(Counter([determine_tense(sent) for sent in txt])) for txt in tgt_tokens]\n",
    "\n",
    "        # determines sentiment value based on custom strings\n",
    "        def custom_sent(sentence):\n",
    "            sent_vals = {\"pos\":0, \"neg\":0}\n",
    "            for word in pos_strings:\n",
    "                if word in sentence.lower():\n",
    "                    sent_vals['pos'] += 1\n",
    "            for word in neg_strings:\n",
    "                if word in sentence.lower():\n",
    "                    sent_vals['neg'] += 1\n",
    "\n",
    "            total = sent_vals['pos'] + sent_vals['neg']\n",
    "\n",
    "            if total > 0:\n",
    "                return (sent_vals['pos'] + -1 * (sent_vals['neg'])) / total\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        mean_custom_scores = [mean([custom_sent(sent) for sent in txt]) for txt in tgt_tokens]\n",
    "\n",
    "        df['overall_sent_comp'] = overall_sent_comp\n",
    "        df['overall_sent_pos'] = overall_sent_pos\n",
    "        df['overall_sent_neg'] = overall_sent_neg\n",
    "        df['mean_t_comp_score'] = mean_t_comp_scores\n",
    "        df['mean_t_pos_score'] = mean_t_pos_scores\n",
    "        df['mean_t_neg_score'] = mean_t_neg_scores\n",
    "        df['mean_tgt_comp_score'] = mean_tgt_comp_scores\n",
    "        df['mean_tgt_pos_score'] = mean_tgt_pos_scores\n",
    "        df['mean_tgt_neg_score'] = mean_tgt_neg_scores\n",
    "\n",
    "        df['verb_tense'] = verb_tenses\n",
    "        df['mean_custom_score'] = mean_custom_scores\n",
    "        return df\n",
    "    \n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_sql(\"select * from \\\"comments\\\"\", dbConnection, chunksize = 10000):\n",
    "\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...\\n', end='')\n",
    "\n",
    "        data = transform_to_many(data)\n",
    "        data = add_sentiment_data(data)\n",
    "        \n",
    "        data.to_sql(name='sentiment', con=engine, if_exists='append', index=False)\n",
    "        rows_imported += len(data)\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 10000...\n",
      "importing rows 15215 to 25215...\n",
      "importing rows 28131 to 38131...\n",
      "importing rows 39945 to 49945...\n",
      "importing rows 52915 to 62915...\n",
      "importing rows 65042 to 75042...\n",
      "importing rows 76941 to 86941...\n",
      "importing rows 89604 to 99604...\n",
      "importing rows 102145 to 112145...\n",
      "importing rows 115353 to 125353...\n",
      "importing rows 128377 to 138377...\n",
      "importing rows 142087 to 152087...\n",
      "importing rows 155064 to 165064...\n",
      "importing rows 166417 to 176417...\n",
      "importing rows 178521 to 188521...\n",
      "importing rows 190762 to 200762...\n",
      "importing rows 203294 to 213294...\n",
      "importing rows 216323 to 226323...\n",
      "importing rows 227877 to 237877...\n",
      "importing rows 240246 to 250246...\n",
      "importing rows 253542 to 263542...\n",
      "importing rows 281523 to 291523...\n",
      "importing rows 295426 to 305426...\n",
      "importing rows 308838 to 318838...\n",
      "importing rows 320200 to 330200...\n",
      "importing rows 332077 to 342077...\n",
      "importing rows 344096 to 354096...\n",
      "importing rows 356552 to 366552...\n",
      "importing rows 369148 to 379148...\n",
      "importing rows 380814 to 390814...\n",
      "importing rows 392647 to 402647...\n",
      "importing rows 406037 to 416037...\n",
      "importing rows 433325 to 443325...\n",
      "importing rows 444545 to 454545...\n",
      "importing rows 457317 to 467317...\n",
      "importing rows 473227 to 483227...\n",
      "importing rows 487707 to 497707...\n",
      "importing rows 501239 to 511239...\n",
      "importing rows 515226 to 525226...\n",
      "importing rows 528744 to 538744...\n"
     ]
    }
   ],
   "source": [
    "sentiment_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094804be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a76387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependecies\n",
    "import pandas as pd\n",
    "from psaw import PushshiftAPI \n",
    "import datetime as dt\n",
    "from config import db_password\n",
    "from config import db_address\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ecf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5e1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to scrape reddit by date\n",
    "def ticker_reddit_scraper(ticker,year,month,day):\n",
    "    #querie reddit for ticker\n",
    "    gen = api.search_comments(q=ticker, \n",
    "                              subreddit='wallstreetbets',\n",
    "                              after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                              before=int(dt.datetime(year, month, day+1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "    #max allowed retrieved responses to ensure function doesn't take too long\n",
    "    max_response=10000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of tickers\n",
    "tickers=['BB', 'CRSR', 'TG', 'WEBR', 'ABNB', 'OI', 'CHGG', 'INTC', 'HP', 'SQ',\n",
    "         'ROKU', 'PT', 'VXX', 'BYND', 'JBLU', 'GBTC',\n",
    "         'MS', 'DKNG', 'FCF', 'GE', 'BJ', 'LCID',  'CS', 'KO',\n",
    "         'WTI', 'RIG', 'RC', 'BA', 'MMAT', 'RKLB', 'UNH', 'RE', \n",
    "         'CSCO', 'VC', 'ZM', 'TIL', 'IBKR', 'CRM', 'IBM', 'ET', 'CP',\n",
    "         'MSTR', 'DAC', 'DTC', 'NYT', 'TH', 'BP', 'LMT', 'SM', \n",
    "         'ITT', 'BBBY', 'TTD', 'PBR', 'SPCE', 'UPST', 'AFRM', 'NG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3047b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because the scraper took so long I had to occasionaly interupt the scraping\n",
    "#so as not to redo completed tickers this list was made\n",
    "tickers_done=['GME', 'TSLA', 'AAPL', 'TWTR', 'ATER', 'AMC','AMD', \n",
    "         'MSFT', 'HD', 'FB', 'AMZN', 'NU', 'DTE', 'GOOG',\n",
    "         'NFLX', 'SOFI', 'TA', 'BBIG', 'NVDA', 'AI', 'ZIM','VTNR',\n",
    "         'PLTR', 'RIVN', 'LUNA', 'TTWO', 'OXY', 'BABA',  'WMT',\n",
    "         'DWAC', 'CC', 'COO', 'LFG', 'RBLX', 'DIS', 'SE', 'SNDL', 'PARA',\n",
    "         'SP', 'XOM', 'ES', 'JD', 'PTON', 'WBD', 'TGT', 'OG',\n",
    "         'DM', 'DOW', 'TLRY', 'NIO', 'PANW', 'CVNA', 'JPM', 'IP', 'TD', 'GS', \n",
    "         'CLOV', 'PYPL', 'GM', 'IQ','MCD', 'MULN', 'DE', 'VZ', 'WW', \n",
    "         'EA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d184737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 524\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 522\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    }
   ],
   "source": [
    "#initial test scraping only gets the first 12 days\n",
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in months:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month,day)\n",
    "                if csv.empty != True:\n",
    "                    #writes scraped data to csv named ticker_year_month_day\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e22e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 522\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    }
   ],
   "source": [
    "#gets the rest of the majority of the month except the ends of the month\n",
    "#due to an error in how the scraper function was written\n",
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "days = [13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month,day)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3042ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite the scraper function so the ends of the month can be taken\n",
    "#removes the API call which \n",
    "def ticker_reddit_scraper_ends(gen):\n",
    "    \n",
    "    #max responses retrieved\n",
    "    max_response=10000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940f274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the ends of the month\n",
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "days = [28,29,30,31]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                #checks if it's the end of month for all months except december\n",
    "                if (month == 2 and day==28)|(month in [4,6,9,11] and day == 30)|(month in [1,3,5,7,8,10] and day == 31):\n",
    "                    gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, month+1, 1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                #checks if it's december\n",
    "                elif month ==12 and day ==31:\n",
    "                    gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, 1, 1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                else:\n",
    "                    try:\n",
    "                        gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, month, day+1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                    except:\n",
    "                        continue\n",
    "                csv = ticker_reddit_scraper_ends(gen)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv',index=False)#querie reddit for ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f734a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrites scraper function for r/stocks subreddit\n",
    "def ticker_reddit_scraper(ticker,year,month):\n",
    "    #querie reddit for ticker\n",
    "    gen = api.search_comments(q=ticker, \n",
    "                              subreddit='stocks',\n",
    "                              after=int(dt.datetime(year, month,1).timestamp()),\n",
    "                              before=int(dt.datetime(year, month+1,1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "    #max allowed retrieved responses to ensure not to long\n",
    "    max_response=310000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33f538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates engine to connect to AWS server\n",
    "db_string = f\"postgresql://postgres:{db_password}@{db_address}:5432/Stockdata\"\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8433109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    }
   ],
   "source": [
    "#scrapes r/stock by month except december\n",
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11]\n",
    "for ticker in tickers+tickers_done:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month)\n",
    "                if csv.empty != True:\n",
    "                    #tries to write AWS if not writes csv\n",
    "                    try:\n",
    "                        csv.to_sql(name='comments_posts', \n",
    "                              con=engine,\n",
    "                              if_exists='append',\n",
    "                              index=False,\n",
    "                              chunksize=5000\n",
    "                             )\n",
    "                    except:\n",
    "                        csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_stocks.csv')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54dd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets december\n",
    "years = [2021,2020,2019,2018]\n",
    "for ticker in tickers+tickers_done:\n",
    "    for year in years:\n",
    "        gen = api.search_comments(q=ticker, \n",
    "                              subreddit='stocks',\n",
    "                              after=int(dt.datetime(year, 12,1).timestamp()),\n",
    "                              before=int(dt.datetime(year+1, 1,1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "        cache=[]\n",
    "        for i in gen:\n",
    "            #take only the dictionary from queries\n",
    "            cache.append(i[-1])\n",
    "        df = pd.DataFrame(cache)\n",
    "        if df.empty != True:\n",
    "            try:\n",
    "                df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "            except KeyError:\n",
    "                df =df\n",
    "            df = df.drop(columns=['created'])\n",
    "            df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "            df=df[['date','body','username','subreddit']]\n",
    "            if df.empty != True:\n",
    "                    try:\n",
    "                        df.to_sql(name='comments_posts', \n",
    "                              con=engine,\n",
    "                              if_exists='append',\n",
    "                              index=False,\n",
    "                              chunksize=5000\n",
    "                             )\n",
    "                    except:\n",
    "                        df.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_stocks.csv')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81951b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload file that failed to upload when it was pulled\n",
    "df = pd.read_csv('csv/re_2020_11_stocks.csv',index_col=0)\n",
    "df.to_sql(name='comments_posts', \n",
    "                              con=engine,\n",
    "                              if_exists='append',\n",
    "                              index=False,\n",
    "                              chunksize=50\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44327b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

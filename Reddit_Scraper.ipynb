{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a76387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependecies\n",
    "#import praw  #not being used\n",
    "import pandas as pd\n",
    "#import config #not being used\n",
    "from psaw import PushshiftAPI \n",
    "import datetime as dt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ecf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5e1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reddit_scraper(ticker,year,month,day):\n",
    "    #querie reddit for ticker\n",
    "    gen = api.search_comments(q=ticker, \n",
    "                              subreddit='wallstreetbets',\n",
    "                              after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                              before=int(dt.datetime(year, month, day+1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "    #max responses retrieved\n",
    "    max_response=10000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=['BB', 'CRSR', 'TG', 'WEBR', 'ABNB', 'OI', 'CHGG', 'INTC', 'HP', 'SQ',\n",
    "         'ROKU', 'PT', 'VXX', 'BYND', 'JBLU', 'GBTC',\n",
    "         'MS', 'DKNG', 'FCF', 'GE', 'BJ', 'LCID',  'CS', 'KO',\n",
    "         'WTI', 'RIG', 'RC', 'BA', 'MMAT', 'RKLB', 'UNH', 'RE', \n",
    "         'CSCO', 'VC', 'ZM', 'TIL', 'IBKR', 'CRM', 'IBM', 'ET', 'CP',\n",
    "         'MSTR', 'DAC', 'DTC', 'NYT', 'TH', 'BP', 'LMT', 'SM', \n",
    "         'ITT', 'BBBY', 'TTD', 'PBR', 'SPCE', 'UPST', 'AFRM', 'NG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8828dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tickers=['UP','YOU', 'IS', 'IT', 'FOR','HE', 'PLAN','BOIL','POST','EVER',\n",
    "              'WOW','FANG','EDIT','RIDE','REAL', 'LOW','ANY','DRIP', 'ST', \n",
    "              'FL','OPEN','NEXT','HOOD','OUT','ICE','BIG','SNOW','HAS','SHOP', \n",
    "              'ALL','AMP','DASH','GO','ON','BE','DNA','NET','AM','CD','PAY',\n",
    "              'ME','UFO', 'USB','SAVE', 'WE','NOW','OR','COIN','MA','SO','CAN',\n",
    "              'MAX','HR','WISH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3047b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_done=['GME', 'TSLA', 'AAPL', 'TWTR', 'ATER', 'AMC','AMD', \n",
    "         'MSFT', 'HD', 'FB', 'AMZN', 'NU', 'DTE', 'GOOG',\n",
    "         'NFLX', 'SOFI', 'TA', 'BBIG', 'NVDA', 'AI', 'ZIM','VTNR',\n",
    "         'PLTR', 'RIVN', 'LUNA', 'TTWO', 'OXY', 'BABA',  'WMT',\n",
    "         'DWAC', 'CC', 'COO', 'LFG', 'RBLX', 'DIS', 'SE', 'SNDL', 'PARA',\n",
    "         'SP', 'XOM', 'ES', 'JD', 'PTON', 'WBD', 'TGT', 'OG',\n",
    "         'DM', 'DOW', 'TLRY', 'NIO', 'PANW', 'CVNA', 'JPM', 'IP', 'TD', 'GS', \n",
    "         'CLOV', 'PYPL', 'GM', 'IQ','MCD', 'MULN', 'DE', 'VZ', 'WW', \n",
    "         'EA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d184737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 524\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 522\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    }
   ],
   "source": [
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in months:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month,day)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e22e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 502\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "C:\\Users\\aleja\\.conda\\envs\\PythonData\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 522\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n"
     ]
    }
   ],
   "source": [
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "days = [13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month,day)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3042ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reddit_scraper_ends(gen):\n",
    "    \n",
    "    #max responses retrieved\n",
    "    max_response=10000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940f274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "days = [28,29,30,31]\n",
    "for ticker in tickers:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                if (month == 2 and day==28)|(month in [4,6,9,11] and day == 30)|(month in [1,3,5,7,8,10] and day == 31):\n",
    "                    gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, month+1, 1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                elif month ==12 and day ==31:\n",
    "                    gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, 1, 1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                else:\n",
    "                    try:\n",
    "                        gen = api.search_comments(q=ticker, \n",
    "                                  subreddit='wallstreetbets',\n",
    "                                  after=int(dt.datetime(year, month, day).timestamp()),\n",
    "                                  before=int(dt.datetime(year, month, day+1).timestamp()),\n",
    "                                  filter=['body','author','subreddit']\n",
    "                                 )\n",
    "                    except:\n",
    "                        continue\n",
    "                csv = ticker_reddit_scraper_ends(gen)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_'+str(day)+'.csv',index=False)#querie reddit for ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f734a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_reddit_scraper(ticker,year,month):\n",
    "    #querie reddit for ticker\n",
    "    gen = api.search_comments(q=ticker, \n",
    "                              subreddit='stocks',\n",
    "                              after=int(dt.datetime(year, month,1).timestamp()),\n",
    "                              before=int(dt.datetime(year, month+1,1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "    #max responses retrieved\n",
    "    max_response=310000\n",
    "    cache=[]\n",
    "    for i in gen:\n",
    "        #take only the dictionary from queries\n",
    "        cache.append(i[-1])\n",
    "        if len(cache) >= max_response:\n",
    "            break\n",
    "    df = pd.DataFrame(cache)\n",
    "    if df.empty != True:\n",
    "        try:\n",
    "            df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "        except KeyError:\n",
    "            return df\n",
    "        df = df.drop(columns=['created'])\n",
    "        df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "        df=df[['date','body','username','subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8433109",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2021,2020,2019,2018]\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11]\n",
    "for ticker in tickers+tickers_done:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "                csv = ticker_reddit_scraper(ticker,year,month)\n",
    "                if csv.empty != True:\n",
    "                    csv.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(month)+'_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c54dd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2021,2020,2019,2018]\n",
    "for ticker in tickers+tickers_done:\n",
    "    for year in years:\n",
    "        gen = api.search_comments(q=ticker, \n",
    "                              subreddit='stocks',\n",
    "                              after=int(dt.datetime(year, 12,1).timestamp()),\n",
    "                              before=int(dt.datetime(year+1, 1,1).timestamp()),\n",
    "                              filter=['body','author','subreddit']\n",
    "                             )\n",
    "        cache=[]\n",
    "        for i in gen:\n",
    "            #take only the dictionary from queries\n",
    "            cache.append(i[-1])\n",
    "        df = pd.DataFrame(cache)\n",
    "        if df.empty != True:\n",
    "            try:\n",
    "                df['created_utc']=pd.to_datetime(df['created_utc'],unit='s').dt.date\n",
    "            except KeyError:\n",
    "                df =df\n",
    "            df = df.drop(columns=['created'])\n",
    "            df.rename(columns={'author':'username','created_utc':'date'}, inplace=True)\n",
    "            df=df[['date','body','username','subreddit']]\n",
    "            if df.empty != True:\n",
    "                        df.to_csv('csv/'+ticker+'_'+str(year)+'_'+str(12)+'_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f538a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
